{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efcc788",
   "metadata": {},
   "source": [
    "# Part 4: Model Context Protocol (MCP)\n",
    "\n",
    "The Model Context Protocol (MCP) is an open standard for connecting AI assistants to external data sources and tools. It enables seamless integration between LLMs and various services, databases, and APIs through a standardized protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca1bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "else:\n",
    "    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY',None)\n",
    "\n",
    "# Create client with api key\n",
    "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fb3e4",
   "metadata": {},
   "source": [
    "## What is MCP?\n",
    "\n",
    "Model Context Protocol (MCP) is a revolutionary approach to extending AI capabilities. Unlike traditional function calling where you define functions locally in your code, MCP allows AI models to connect to remote servers that provide tools and resources.\n",
    "\n",
    "\n",
    "- **🔌 Plug-and-Play Integration**: Connect to any MCP-compatible service instantly\n",
    "- **🌐 Remote Capabilities**: Access tools and data from anywhere on the internet\n",
    "- **🔄 Standardized Protocol**: One protocol works with all MCP servers\n",
    "- **🔒 Centralized Security**: Control access and permissions at the server level\n",
    "- **📈 Scalability**: Share resources across multiple AI applications\n",
    "- **🛠️ Rich Ecosystem**: Growing library of MCP servers for various use case\n",
    "\n",
    "## 1. Working with Stdio MCP Servers\n",
    "\n",
    "Stdio (Standard Input/Output) servers run as local processes and communicate through pipes. This is perfect for:\n",
    "- Development and testing\n",
    "- Local tools and utilities\n",
    "- Lightweight integrations\n",
    "\n",
    "\n",
    "## 1. Working with MCP Servers\n",
    "\n",
    "Let's use the DeepWiki MCP server, which provides access to Wikipedia data and search capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ee5d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. The weather in London on 2025-05-30 will be: The temperature in Celsius will be 15.6 at 00:00, 15.3 at 01:00, 15.3 at 02:00, 15.3 at 03:00, 15.1 at 04:00, 15.3 at 05:00, 14.6 at 06:00, 15.7 at 07:00, 17 at 08:00, 17.8 at 09:00, 19.4 at 10:00, 20.9 at 11:00, 22.1 at 12:00, 23.3 at 13:00, 24 at 14:00, 23.7 at 15:00, 23.7 at 16:00, 23.1 at 17:00, 22.8 at 18:00, 21 at 19:00, 20.2 at 20:00, 19.3 at 21:00, 18.5 at 22:00, 17.9 at 23:00.\n"
     ]
    }
   ],
   "source": [
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",  # Executable\n",
    "    args=[\"-y\", \"@philschmid/weather-mcp\"],  # MCP Server\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Prompt to get the weather for the current day in London.\n",
    "            prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n",
    "            # Initialize the connection between client and server\n",
    "            await session.initialize()\n",
    "            # Send request to the model with MCP function declarations\n",
    "            response = await client.aio.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=[session],  # uses the session, will automatically call the tool\n",
    "                    # Uncomment if you **don't** want the sdk to automatically call the tool\n",
    "                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(\n",
    "                    #     disable=True\n",
    "                    # ),\n",
    "                ),\n",
    "            )\n",
    "            print(response.text)\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4ac07",
   "metadata": {},
   "source": [
    "## !! Exercise: Build Your Own MCP CLI Agent !!\n",
    "\n",
    "Create an interactive CLI chat agent that uses the DeepWiki MCP server to answer about Github repositories.\n",
    "\n",
    "Task:\n",
    "- Create an interactive CLI chat agent that uses the DeepWiki MCP server to answer about Github repositories.\n",
    "- Remote server: https://mcp.deepwiki.com/mcp\n",
    "- use `streamablehttp_client` to connect to the server\n",
    "- Use the DeepWiki MCP server to search for information about Github repositories.\n",
    "- Use the DeepWiki MCP server to search for information about Github repositories.\n",
    "\n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "- Install required packages: \n",
    "- Python 3.13+ required\n",
    "- Interactive chat loop with exit functionality\n",
    "- Display function calls and responses for transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae372605",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_url = \"https://mcp.deepwiki.com/mcp\"\n",
    "\n",
    "async def run():\n",
    "    async with streamablehttp_client(remote_url) as (read, write, _):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            # Initialize conversation history using simple tuples\n",
    "            config = genai.types.GenerateContentConfig(\n",
    "                temperature=0,\n",
    "                tools=[session],\n",
    "            )\n",
    "            print(\"Agent is ready. Type 'exit' to quit.\")\n",
    "            chat = client.aio.chats.create(model=\"gemini-2.5-flash-preview-05-20\", config=config)\n",
    "            while True:\n",
    "                user_input = input(\"You: \")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    print(\"Exiting chat.\")\n",
    "                    break\n",
    "\n",
    "                # Append user message to history\n",
    "                response = await chat.send_message(user_input)\n",
    "                if len(response.automatic_function_calling_history) > 0:\n",
    "                    if (\n",
    "                        response.automatic_function_calling_history[0].parts[0].text\n",
    "                        == user_input\n",
    "                    ):\n",
    "                        response.automatic_function_calling_history.pop(0)\n",
    "                    for call in response.automatic_function_calling_history:\n",
    "                        if call.parts[0].function_call:\n",
    "                            print(f\"Function call: {call.parts[0].function_call}\")\n",
    "                        elif call.parts[0].function_response:\n",
    "                            print(\n",
    "                                f\"Function response: {call.parts[0].function_response.response['result'].content[0].text}\"\n",
    "                            )\n",
    "                print(f\"Assistant: {response.text}\")\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe3000",
   "metadata": {},
   "source": [
    "## Recap & Next Steps\n",
    "\n",
    "**What You've Learned:**\n",
    "- Understanding the Model Context Protocol (MCP) and its advantages over traditional function calling\n",
    "- Connecting to remote MCP servers using both stdio and HTTP protocols\n",
    "- Building interactive chat agents that leverage MCP capabilities\n",
    "\n",
    "**Key Takeaways:**\n",
    "- MCP enables plug-and-play integration with external services and data sources\n",
    "- Remote capabilities provide access to tools and data from anywhere on the internet\n",
    "- Standardized protocols ensure compatibility across different AI applications\n",
    "- Centralized security and permissions improve enterprise deployment scenarios\n",
    "- The MCP ecosystem is rapidly growing with servers for various use cases\n",
    "\n",
    "🎉 **Congratulations!** You've completed the Gemini 2.5 AI Engineering Workshop\n",
    "\n",
    "**More Resources:**\n",
    "- [MCP with Gemini Documentation](https://ai.google.dev/gemini-api/docs/function-calling?example=weather#model_context_protocol_mcp)\n",
    "- [Function Calling Documentation](https://ai.google.dev/gemini-api/docs/function-calling?lang=python)\n",
    "- [MCP Official Specification](https://spec.modelcontextprotocol.io/)\n",
    "- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n",
    "- [MCP Server Directory](https://github.com/modelcontextprotocol/servers)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
